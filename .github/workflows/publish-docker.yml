name: Publish Docker images

on:
  release:
    types: [published]
  workflow_dispatch:

permissions:
  packages: write
  contents: read

    
jobs:

  build-llama-cpp:
    uses: localagi/ai-dedicated-workflows/.github/workflows/operation-docker-build-publish.yml@v3
    with:
      dockerfile: Dockerfile
      context-repository: ggerganov/llama.cpp
      context-repository-ref: ${{ github.ref_name }}
      registry-repo-name: llama.cpp
      tags: |
        type=schedule
        type=ref,event=branch
        type=semver,pattern={{raw}}
      flavor: |
        suffix=${{ matrix.suffix }}
      build-args: |
        FROM_IMAGE=${{ matrix.from }}
        CMAKE_ARGS=${{ matrix.cmake-args }}
      platforms: linux/amd64,linux/arm64/v8
    secrets: inherit
    strategy:
      fail-fast: false
      matrix:
        # see https://hub.docker.com/r/nvidia/cuda/tags
        include:
        - blas: CUBLAS
          cmake-args: "-DLLAMA_CUBLAS=on -DCMAKE_CUDA_ARCHITECTURES=61;86"
          suffix: "-cublas-11.7.1" 
          from: nvidia/cuda:11.7.1-devel-ubuntu22.04
          
        - blas: CUBLAS
          cmake-args: "-DLLAMA_CUBLAS=on -DCMAKE_CUDA_ARCHITECTURES=61;86;89"
          suffix: "-cublas-12.1.1"
          from: nvidia/cuda:12.1.1-devel-ubuntu22.04
          
        - blas: OPENBLAS
          from: python:3.10-slim-bullseye
          cmake-args: "-DLLAMA_BLAS=ON -DLLAMA_BLAS_VENDOR=OpenBLAS"
