version: "3.8"
services:

# CPU Inference
  llama.cpp-cpu:
    image: localagi/llama.cpp:master
    command: ["--model", "${LOCAL_MODEL_DIR}/Wizard-Vicuna-13B-Uncensored-GGML/Wizard-Vicuna-13B-Uncensored.ggml.q5_1.bin" ]
    init: true
    tty: true
    volumes:
      - $LOCAL_MODEL_DIR:/models
              
# GPU Inference        
  llama.cpp-gpu:
    image: localagi/llama.cpp:master-cublas-${CUDA_VERSION}
    command: ["--model", "${LOCAL_MODEL_DIR}/Wizard-Vicuna-13B-Uncensored-GGML/Wizard-Vicuna-13B-Uncensored.ggml.q5_1.bin" ]
    init: true
    tty: true
    volumes:
      - $LOCAL_MODEL_DIR:/models
    deploy:
      resources:
        reservations:
          devices:
            - driver: nvidia
              capabilities: [gpu]
