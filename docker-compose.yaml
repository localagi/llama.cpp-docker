version: "3.8"
services:

# GPU Inference        
  llama.cpp-server:
    image: localagi/llama.cpp:${LLAMA_CPP_VERSION}${LLAMA_CPP_FLAVOR}
    command: "server --model ${LOCAL_MODEL_DIR}/Wizard-Vicuna-7B-Uncensored-GGML/Wizard-Vicuna-7B-Uncensored.ggmlv3.q5_1.bin --host 0.0.0.0"
    ports:
      - 8080:8080
    volumes:
      - $LOCAL_MODEL_DIR:$LOCAL_MODEL_DIR
    deploy:
      resources:
        reservations:
          devices:
            - driver: nvidia
              capabilities: [gpu]
