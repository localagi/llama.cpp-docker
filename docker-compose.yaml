version: "3.8"
services:

# GPU Inference        
  llama.cpp-server:
    image: localagi/llama.cpp:${LLAMA_CPP_VERSION}${LLAMA_CPP_FLAVOR}
    command: ["--model", "/models/Wizard-Vicuna-13B-Uncensored-GGML/Wizard-Vicuna-13B-Uncensored.ggml.q5_1.bin" ]
    init: true
    tty: true
    volumes:
      - $LOCAL_MODEL_DIR:/models
    deploy:
      resources:
        reservations:
          devices:
            - driver: nvidia
              capabilities: [gpu]
